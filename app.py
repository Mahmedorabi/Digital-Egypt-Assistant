import streamlit as st
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
from langchain_openai import ChatOpenAI
from langchain_together import ChatTogether
from langchain_google_genai import ChatGoogleGenerativeAI
import os
from uuid import uuid4

# ========== Page Config ==========
st.set_page_config(page_title="Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø±Ù‚Ù…ÙŠ", layout="wide")
st.title("ğŸ’¬ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø±Ù‚Ù…ÙŠ Ù„Ø®Ø¯Ù…Ø§Øª Ù…ØµØ±")
st.markdown("Ø§Ø³Ø£Ù„Ù†ÙŠ Ø¹Ù† Ø£ÙŠ Ø®Ø¯Ù…Ø© Ø±Ù‚Ù…ÙŠØ© Ù…ØªØ§Ø­Ø© Ø¹Ù„Ù‰ Ø¨ÙˆØ§Ø¨Ø© Ù…ØµØ± Ø§Ù„Ø±Ù‚Ù…ÙŠØ©")
session_id = uuid4()
# ========== Sidebar ==========
st.sidebar.title("ğŸ”§ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
model_choice = st.sidebar.selectbox(
    "Ø§Ø®ØªØ± Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ:",
    ["OpenAI - GPT-4o", "Together - LLaMA 3", "Gemini"]
)
api_key = st.sidebar.text_input("Ø£Ø¯Ø®Ù„ Ù…ÙØªØ§Ø­ API Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ:", type="password")

# ========== Model Initialization ==========
llm = None

try:
    if model_choice == "OpenAI - GPT-4o":
        if api_key:
            os.environ["OPENAI_API_KEY"] = api_key
            llm = ChatOpenAI(model="gpt-4o", temperature=0.0)
            # test it by calling something lightweight
            _ = llm.invoke("Ù…Ø±Ø­Ø¨Ø§")
        else:
            st.warning("ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ù…ÙØªØ§Ø­ OpenAI API ÙÙŠ Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ.")

    elif model_choice == "Together - LLaMA 3":
        if api_key:
            llm = ChatTogether(
                model="meta-llama/Llama-3.3-70B-Instruct-Turbo",
                temperature=0,
                api_key=api_key
            )
            _ = llm.invoke("Ù…Ø±Ø­Ø¨Ø§")
        else:
            st.warning("ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ù…ÙØªØ§Ø­ Together API ÙÙŠ Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ.")

    elif model_choice == "Gemini":
        if api_key:
            os.environ["GOOGLE_API_KEY"] = api_key
            llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0)
            _ = llm.invoke("Ù…Ø±Ø­Ø¨Ø§")
        else:
            st.warning("ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ù…ÙØªØ§Ø­ Google API ÙÙŠ Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ.")

except Exception as e:
    llm = None
    st.error(f"ÙØ´Ù„ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…ÙØªØ§Ø­ API: {str(e)}")

# ========== Embedding and Vector Store ==========
@st.cache_resource
def get_embedding():
    return HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large")

embedding = get_embedding()
vector_store = Chroma(
    embedding_function=embedding,
    persist_directory="infloat"
)
retriever = vector_store.as_retriever(search_kwargs={"k": 10})

# ========== Prompt Template ==========
prompt = ChatPromptTemplate.from_messages([
    ("system", 
     "Ù…Ø±Ø­Ø¨Ù‹Ø§! ğŸ‘‹ Ø£Ù†Ø§ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø±Ù‚Ù…ÙŠ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ ÙÙ‡Ù… ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ø¹Ù„Ù‰ Ø¨ÙˆØ§Ø¨Ø© Ù…ØµØ± Ø§Ù„Ø±Ù‚Ù…ÙŠØ©. "
     "ÙŠØ±Ø¬Ù‰ Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„ÙƒØŒ ÙˆØ³Ø£Ø³ØªØ®Ø±Ø¬ Ù„Ùƒ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø°Ø§Øª ØµÙ„Ø©:\n\n"
     "Ù…Ø¹Ù„ÙˆÙ…Ø© Ù…Ù‡Ù…Ø©: Ø§Ø¹ØªØ¨Ø± Ø£Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ù…ØªØ±Ø§Ø¯ÙØ© ÙˆØªØ¹Ù†ÙŠ Ù†ÙØ³ Ø§Ù„Ø´ÙŠØ¡:\n"
     "- 'Ø§Ù„Ù…Ø±ÙƒØ¨Ø©' Ø£Ùˆ 'Ø§Ù„Ù…Ø±ÙƒØ¨Ù‡' = 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©' Ø£Ùˆ 'Ø§Ù„Ø¹Ø±Ø¨ÙŠÙ‡'\n"
     "- ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙŠÙÙ‘ Ù…Ù†Ù‡Ø§ ÙÙŠ Ø³Ø¤Ø§Ù„Ùƒ ÙˆØ³Ø£ÙÙ‡Ù… Ø§Ù„Ù…Ù‚ØµÙˆØ¯ Ù†ÙØ³Ù‡.\n\n"
     "--------------------\n"
     "ğŸ“Œ **Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ø§Ù…Ø© Ù…Ù† Ø§Ù„Ø¨ÙˆØ§Ø¨Ø©:**\n\n"
     "â–ªï¸ **Ù‡Ù„ ÙŠØ¬Ø¨ ØªÙˆØ¬Ù‡ÙŠ Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù‡Ø© Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠØ© Ù„Ø·Ù„Ø¨ Ø§Ù„Ø®Ø¯Ù…Ø©ØŸ**\n"
     "Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ØªØªØ·Ù„Ø¨ ØªÙˆÙ‚ÙŠØ¹Ùƒ Ø£Ùˆ ÙˆØ¬ÙˆØ¯Ùƒ Ø§Ù„Ø´Ø®ØµÙŠØŒ ÙÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø§Ù„ØªÙˆØ¬Ù‡ Ù„Ù„Ø¬Ù‡Ø© Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠØ© Ø§Ù„ØªØ§Ø¨Ø¹ Ù„Ù‡Ø§ Ø§Ù„Ø®Ø¯Ù…Ø© Ù„Ø¥ØªÙ…Ø§Ù… Ø·Ù„Ø¨Ùƒ...\n\n"
     "â–ªï¸ **Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø¹Ø§Ø¯Ø©Ù‹ØŸ**\n"
     "Ø¨Ø¹Ø¶ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØ±Ù‚ÙŠØ© Ù„Ø§Ø³ØªÙƒÙ…Ø§Ù„Ù‡Ø§...\n\n"
     "â–ªï¸ **Ù‡Ù„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ø¨ÙˆØ§Ø¨Ø© Ù…Ø­Ø¯Ù‘Ø«Ø©ØŸ**\n"
     "Ù†Ø¹Ù…ØŒ ÙŠØªÙ… ØªØ­Ø¯ÙŠØ«Ù‡Ø§ Ø¨ØµÙˆØ±Ø© Ù…ØªÙˆØ§ØµÙ„Ø©.\n\n"
     "â–ªï¸ **Ù‡Ù„ ØªØ­ØªÙˆÙŠ Ø§Ù„Ø¨ÙˆØ§Ø¨Ø© Ø¹Ù„Ù‰ Ù‚Ø³Ù… Ù…Ø³Ø§Ø¹Ø¯Ø© Ø£Ùˆ Ø£Ø³Ø¦Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø©ØŸ**\n"
     "Ù†Ø¹Ù…ØŒ ÙŠÙˆØ¬Ø¯ Ù‚Ø³Ù… Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙˆØ§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©.\n\n"
     "â–ªï¸ **Ù‡Ù„ ÙŠÙˆØ¬Ø¯ Ù…Ø±Ø§ÙƒØ² Ø®Ø¯Ù…Ø© Ø£Ù… ÙŠØªÙˆØ¬Ø¨ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ù„Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ ÙÙ‚Ø·ØŸ**\n"
     "ØªÙˆØ¬Ø¯ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø¹Ø¨Ø± Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø£Ùˆ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø£Ùˆ Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø®Ø¯Ù…Ø© Ø£Ùˆ Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ù‡Ø§ØªÙÙŠ.\n\n"
     "ğŸ“ **Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø·ÙˆØ§Ø±Ø¦ Ø§Ù„Ù…Ù‡Ù…Ø©:**\n"
     "- 122: Ø§Ù„Ù†Ø¬Ø¯Ø©\n"
     "- 123: Ø§Ù„Ø¥Ø³Ø¹Ø§Ù\n"
     "- 121: Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡\n\n"
     "ğŸ“² **Ø¯Ø¹Ù… Ø¨ÙˆØ§Ø¨Ø© Ù…ØµØ± Ø§Ù„Ø±Ù‚Ù…ÙŠØ©:**\n"
     "- Ø±Ù‚Ù… Ø§Ù„Ø¯Ø¹Ù…: Ù¡Ù¥Ù©Ù©Ù©\n\n"
     "ğŸ’¬ **Ù„Ø¥Ø¶Ø§ÙØ© Ù…Ù„Ø§Ø­Ø¸Ø§ØªÙƒ Ø£Ùˆ ØªÙ‚ÙŠÙŠÙ… ØªØ¬Ø±Ø¨ØªÙƒ Ø§Ùˆ Ø´ÙƒØ§ÙˆÙŠØŒ ØªÙØ¶Ù„ Ø¨Ø²ÙŠØ§Ø±Ø©:**\n"
     "https://digital.gov.eg/feedback\n"
     "--------------------\n"
     "{context}"
     ),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])

# ========== Run App Logic ==========
if llm is not None:
    combine_docs_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)
    retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)
    memory = StreamlitChatMessageHistory()

    chain_with_history = RunnableWithMessageHistory(
        retrieval_chain,
        lambda session_id: memory,
        input_messages_key="input",
        history_messages_key="chat_history"
    )

    # ========== Most Common Services ==========
    st.subheader("ğŸ“ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹")
    common_services = {
        "Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¹Ù† Ù…Ø®Ø§Ù„ÙØ§Øª Ø±Ø®ØµØ© Ù…Ø±ÙƒØ¨Ø©": "Ø£Ø±ÙŠØ¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¹Ù† Ù…Ø®Ø§Ù„ÙØ§Øª Ø±Ø®ØµØ© Ù…Ø±ÙƒØ¨Ø©",
        "Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¹Ù† ØµØ±Ù": "Ø£Ø±ÙŠØ¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¹Ù† ØµØ±Ù Ù…Ø¹ÙŠÙ†",
        "Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¹Ù† Ø£Ø®Ø± Ù…Ø¯Ø© ØªØ£Ù…ÙŠÙ†ÙŠØ©": "Ù…Ø§ Ù‡ÙŠ Ø¢Ø®Ø± Ù…Ø¯Ø© ØªØ£Ù…ÙŠÙ†ÙŠØ© Ù„ÙŠØŸ",
        "Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¹Ù† Ù…Ø¯Ø¯ Ø§Ù„Ø§Ø´ØªØ±Ø§Ùƒ Ùˆ Ø§Ù„Ø§Ø¬ÙˆØ±": "Ø£Ø±ÙŠØ¯ Ù…Ø¹Ø±ÙØ© Ù…Ø¯Ø¯ Ø§Ù„Ø§Ø´ØªØ±Ø§Ùƒ ÙˆØ§Ù„Ø£Ø¬ÙˆØ± Ø§Ù„Ø®Ø§ØµØ© Ø¨ÙƒÙ„ Ù…Ø¯Ø© ÙÙŠ Ø§Ù„ØªØ£Ù…ÙŠÙ† Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ",
        "Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¹Ù† Ø§Ù„Ø±Ù‚Ù… Ø§Ù„ØªØ£Ù…ÙŠÙ†ÙŠ": "Ù…Ø§ Ù‡Ùˆ Ø±Ù‚Ù…ÙŠ Ø§Ù„ØªØ£Ù…ÙŠÙ†ÙŠØŸ"
    }

    cols = st.columns(len(common_services))
    for i, (label, query) in enumerate(common_services.items()):
        if cols[i].button(label):
            st.session_state["preset_query"] = query

    # ========== Chat UI ==========
    default_input = st.session_state.pop("preset_query", "") if "preset_query" in st.session_state else None
    user_input = st.chat_input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§...")
    if user_input is None and default_input:
        user_input = default_input

    # Display message history
    for msg in memory.messages:
        role = "user" if msg.type == "human" else "assistant"
        with st.chat_message(role):
            st.markdown(msg.content)

    if user_input:
        st.chat_message("user").markdown(user_input)

        with st.chat_message("assistant"):
            full_answer = ""
            placeholder = st.empty()

            for chunk in chain_with_history.stream(
                {"input": user_input},
                config={"configurable": {"session_id": str(session_id)}}
            ):
                if isinstance(chunk, dict) and "answer" in chunk:
                    full_answer += chunk["answer"]
                    placeholder.markdown(full_answer + "â–Œ")

            placeholder.markdown(full_answer)
            memory.add_user_message(user_input)
            memory.add_ai_message(full_answer)
